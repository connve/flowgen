flow:
  name: bigquery_query_basic
  require_leader_election: true
  tasks:

    # Generate triggers the flow once after 10 seconds.
    - generate:
        name: trigger
        interval: 10s
        count: 1

    # BigQuery query executes a simple SELECT query.
    # Results are returned as Arrow RecordBatch for efficient columnar processing.
    - gcp_bigquery_query:
        name: fetch_orders
        # Path to GCP service account JSON credentials
        credentials_path: /etc/gcp/credentials.json
        # GCP project ID where BigQuery resources are located
        project_id: my-project-id
        # SQL query (inline or resource file)
        query: "SELECT id, customer_id, amount, order_date FROM `my-project-id.analytics.orders` WHERE order_date >= '2024-01-01' ORDER BY order_date DESC"
        # Optional: Maximum number of rows to return
        max_results: 10000
        # Optional: Query execution timeout (e.g., "30s", "5m", "1h")
        timeout: "1m"

    # Convert Arrow RecordBatch to JSON format for downstream processing.
    - convert:
        name: arrow_to_json
        target_format: json

    # Split JSON array into individual row events for per-record processing.
    - iterate:
        name: split_rows

    # Output individual row events to console for debugging and monitoring.
    - log:
        name: log
        level: info
        structured: false
