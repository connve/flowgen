flow:
  require_leader_election: true
  name: read_file_with_buffer
  tasks:
    # Generate triggers the flow once after 10 seconds.
    - generate:
        name: trigger
        interval: 10s
        count: 1

    # Object store read reads CSV file with 10,000 order records.
    - object_store_read:
        name: read_file
        path: file:///flowgen/examples/data/orders.csv
        batch_size: 1000
        has_header: true

    # Convert transforms CSV data to JSON format.
    - convert:
        name: to_json
        target_format: json

    # Iterate splits the batch into individual row events (fan-out).
    - iterate:
        name: split_rows

    # Buffer accumulates individual events into batches of 75 before forwarding downstream.
    # The partition_key parameter is optional and groups events by rendered template value.
    # Each partition maintains independent buffers that flush separately based on size and timeout.
    # You can use event.data.* or event.meta.* in the partition_key template.
    - buffer:
        name: group_into_batches
        size: 75
        partition_key: "{{event.data.payment_type}}"
        timeout: "30s"

    # Script unwraps the batch and adds metadata for downstream tasks.
    # The _meta field is extracted separately and available in templates as event.meta.*.
    - script:
        name: unwrap_batch
        code: |
          #{
            "data": event.data.batch,
            "_meta": #{
              "partition_key": event.data.partition_key
            }
          }

    # NATS JetStream publisher publishes the unwrapped batch array to the message stream.
    - nats_jetstream_publisher:
        name: publish_batches
        credentials_path: /etc/nats/credentials.json
        url: "localhost:4222"
        subject: rows.received_batched
        stream:
          create_or_update: true
          name: file
          description: "All general file drops"
          subjects: ["rows.>"]
          max_age_secs: 86400
          retention: limits
          discard: old
